{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5bfbb81-41a6-4542-a0af-2f86ef7c6f08",
   "metadata": {},
   "source": [
    "## Lab 06: Data Lakehouse with Structured Streaming\n",
    "This lab will help you learn to use many of the software libraries and programming techniques required to fulfill the requirements of the final end-of-session capstone project for course **DS-2002: Data Systems**. The spirit of the project is to provide a capstone challenge that requires students to demonstrate a practical and functional understanding of each of the data systems and architectural principles covered throughout the session.\n",
    "\n",
    "**These include:**\n",
    "- Relational Database Management Systems (e.g., MySQL, Microsoft SQL Server, Oracle, IBM DB2)\n",
    "  - Online Transaction Processing Systems (OLTP): *Optimized for High-Volume Write Operations; Normalized to 3rd Normal Form.*\n",
    "  - Online Analytical Processing Systems (OLAP): *Optimized for Read/Aggregation Operations; Dimensional Model (i.e, Star Schema)*\n",
    "- NoSQL *(Not Only SQL)* Systems (e.g., MongoDB, CosmosDB, Cassandra, HBase, Redis)\n",
    "- File System *(Data Lake)* Source Systems (e.g., AWS S3, Microsoft Azure Data Lake Storage)\n",
    "  - Various Datafile Formats (e.g., JSON, CSV, Parquet, Text, Binary)\n",
    "- Massively Parallel Processing *(MPP)* Data Integration Systems (e.g., Apache Spark, Databricks)\n",
    "- Data Integration Patterns (e.g., Extract-Transform-Load, Extract-Load-Transform, Extract-Load-Transform-Load, Lambda & Kappa Architectures)\n",
    "\n",
    "### Section I: Prerequisites\n",
    "\n",
    "#### 1.0. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d50f5aa-9196-4f40-b0b7-c5d3cb37be3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\spark-3.5.4-bin-hadoop3\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print(findspark.find())\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import pymongo\n",
    "import certifi\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window as W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45146da3-cf8f-43f7-8aa1-d2dc6822a4cf",
   "metadata": {},
   "source": [
    "#### 2.0. Instantiate Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c4b954-2cee-47ec-abe3-ac2211d6e9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Specify MySQL Server Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "mysql_args = {\n",
    "    \"host_name\" : \"localhost\",\n",
    "    \"port\" : \"3306\",\n",
    "    \"db_name\" : \"estella_dw\",  \n",
    "    \"conn_props\" : {\n",
    "        \"user\" : \"root\",\n",
    "        \"password\" : \"DwightSchrute21!\",  \n",
    "        \"driver\" : \"com.mysql.cj.jdbc.Driver\"\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify MongoDB Cluster Connection Information\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "mongodb_args = {\n",
    "    \"cluster_location\" : \"atlas\",\n",
    "    \"user_name\" : \"stellarhill\",\n",
    "    \"password\" : \"DwightSchrute21\",\n",
    "    \"cluster_name\" : \"StellCluster\",\n",
    "    \"cluster_subnet\" : \"p4yt3yg\",\n",
    "    \"db_name\" : \"estella_dw\",  \n",
    "    \"collection\" : \"\",\n",
    "    \"null_column_threshold\" : 0.5\n",
    "}\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Specify Directory Structure for Source Data\n",
    "# --------------------------------------------------------------------------------\n",
    "base_dir = os.path.join(os.getcwd(), 'lab_data')\n",
    "data_dir = os.path.join(base_dir, 'estella_dw')\n",
    "batch_dir = os.path.join(data_dir, 'batch')\n",
    "stream_dir = os.path.join(data_dir, 'streaming')\n",
    "\n",
    "fact_sales_stream_dir = os.path.join(stream_dir, 'fact_sales')\n",
    "\n",
    "# --------------------------------------------------------------------------------\n",
    "# Create Directory Structure for Data Lakehouse Files\n",
    "# --------------------------------------------------------------------------------\n",
    "dest_database = \"estella_dw\"\n",
    "sql_warehouse_dir = os.path.abspath('spark-warehouse')\n",
    "dest_database_dir = f\"{dest_database}.db\"\n",
    "database_dir = os.path.join(sql_warehouse_dir, dest_database_dir)\n",
    "\n",
    "fact_sales_output_bronze = os.path.join(database_dir, 'fact_sales', 'bronze')\n",
    "fact_sales_output_silver = os.path.join(database_dir, 'fact_sales', 'silver')\n",
    "fact_sales_output_gold = os.path.join(database_dir, 'fact_sales', 'gold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0c93a0c-b5ba-413f-b4cc-534ef7e09a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch dir: C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\batch\n",
      "Fact sales stream dir: C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\streaming\\fact_sales\n",
      "Warehouse dir: C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\spark-warehouse\\estella_dw.db\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch dir:\", batch_dir)\n",
    "print(\"Fact sales stream dir:\", fact_sales_stream_dir)\n",
    "print(\"Warehouse dir:\", database_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ce09c3-3805-4661-917c-c4ff630d940c",
   "metadata": {},
   "source": [
    "### 3.0. Define Global Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "773a4cd2-c8da-4e9a-ac18-a3f99bb6b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(path: str):\n",
    "    file_sizes = []\n",
    "    modification_times = []\n",
    "\n",
    "    '''Fetch each item in the directory, and filter out any directories.'''\n",
    "    items = os.listdir(path)\n",
    "    files = sorted([item for item in items if os.path.isfile(os.path.join(path, item))])\n",
    "\n",
    "    '''Populate lists with the Size and Last Modification DateTime for each file in the directory.'''\n",
    "    for file in files:\n",
    "        file_sizes.append(os.path.getsize(os.path.join(path, file)))\n",
    "        modification_times.append(pd.to_datetime(os.path.getmtime(os.path.join(path, file)), unit='s'))\n",
    "\n",
    "    data = list(zip(files, file_sizes, modification_times))\n",
    "    column_names = ['name','size','modification_time']\n",
    "    \n",
    "    return pd.DataFrame(data=data, columns=column_names)\n",
    "\n",
    "\n",
    "def wait_until_stream_is_ready(query, min_batches=1):\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5)\n",
    "        \n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "\n",
    "def remove_directory_tree(path: str):\n",
    "    '''If it exists, remove the entire contents of a directory structure at a given 'path' parameter's location.'''\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path)\n",
    "            return f\"Directory '{path}' has been removed successfully.\"\n",
    "        else:\n",
    "            return f\"Directory '{path}' does not exist.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "        \n",
    "\n",
    "def drop_null_columns(df, threshold):\n",
    "    \"\"\"\n",
    "    Drop columns that have a percentage of NULL values exceeding the threshold.\n",
    "    Skip if DataFrame has only _corrupt_record column.\n",
    "    \"\"\"\n",
    "    if df.columns == [\"_corrupt_record\"]:\n",
    "        print(\"Warning: DataFrame only has _corrupt_record column. Skipping drop_null_columns.\")\n",
    "        return df\n",
    "    \n",
    "    total_rows = df.count()\n",
    "    if total_rows == 0:\n",
    "        return df\n",
    "\n",
    "    columns_with_nulls = [\n",
    "        col for col in df.columns if df.filter(df[col].isNull()).count() / total_rows > threshold\n",
    "    ]\n",
    "    df_dropped = df.drop(*columns_with_nulls)\n",
    "    return df_dropped\n",
    "\n",
    "\n",
    "    \n",
    "def get_mysql_dataframe(spark_session, sql_query : str, **args):\n",
    "    '''Create a JDBC URL to the MySQL Database'''\n",
    "    jdbc_url = f\"jdbc:mysql://{args['host_name']}:{args['port']}/{args['db_name']}\"\n",
    "    \n",
    "    '''Invoke the spark.read.format(\"jdbc\") function to query the database, and fill a DataFrame.'''\n",
    "    dframe = spark_session.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", jdbc_url) \\\n",
    "    .option(\"driver\", args['conn_props']['driver']) \\\n",
    "    .option(\"user\", args['conn_props']['user']) \\\n",
    "    .option(\"password\", args['conn_props']['password']) \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .load()\n",
    "    \n",
    "    return dframe\n",
    "    \n",
    "\n",
    "def get_mongo_uri(**args):\n",
    "    '''Validate proper input'''\n",
    "    if args[\"cluster_location\"] not in ['atlas', 'local']:\n",
    "        raise Exception(\"You must specify either 'atlas' or 'local' for the 'cluster_location' parameter.\")\n",
    "        \n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        uri = f\"mongodb+srv://{args['user_name']}:{args['password']}@\"\n",
    "        uri += f\"{args['cluster_name']}.{args['cluster_subnet']}.mongodb.net/\"\n",
    "    else:\n",
    "        uri = \"mongodb://localhost:27017/\"\n",
    "\n",
    "    return uri\n",
    "\n",
    "\n",
    "def get_spark_conf_args(spark_jars : list, **args):\n",
    "    jars = \"\"\n",
    "    for jar in spark_jars:\n",
    "        jars += f\"{jar}, \"\n",
    "    \n",
    "    sparkConf_args = {\n",
    "        \"app_name\" : \"PySpark Estella Data Lakehouse (Medallion Architecture)\",\n",
    "        \"worker_threads\" : f\"local[{int(os.cpu_count()/2)}]\",\n",
    "        \"shuffle_partitions\" : int(os.cpu_count()),\n",
    "        \"mongo_uri\" : get_mongo_uri(**args),\n",
    "        \"spark_jars\" : jars[0:-2],\n",
    "        \"database_dir\" : sql_warehouse_dir\n",
    "    }\n",
    "    \n",
    "    return sparkConf_args\n",
    "    \n",
    "\n",
    "def get_spark_conf(**args):\n",
    "    sparkConf = SparkConf().setAppName(args['app_name'])\\\n",
    "    .setMaster(args['worker_threads']) \\\n",
    "    .set('spark.driver.memory', '4g') \\\n",
    "    .set('spark.executor.memory', '2g') \\\n",
    "    .set('spark.jars', args['spark_jars']) \\\n",
    "    .set('spark.jars.packages', 'org.mongodb.spark:mongo-spark-connector_2.12:3.0.1') \\\n",
    "    .set('spark.mongodb.input.uri', args['mongo_uri']) \\\n",
    "    .set('spark.mongodb.output.uri', args['mongo_uri']) \\\n",
    "    .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "    .set('spark.sql.debug.maxToStringFields', 35) \\\n",
    "    .set('spark.sql.shuffle.partitions', args['shuffle_partitions']) \\\n",
    "    .set('spark.sql.streaming.forceDeleteTempCheckpointLocation', 'true') \\\n",
    "    .set('spark.sql.streaming.schemaInference', 'true') \\\n",
    "    .set('spark.sql.warehouse.dir', args['database_dir']) \\\n",
    "    .set('spark.streaming.stopGracefullyOnShutdown', 'true')\n",
    "    \n",
    "    return sparkConf\n",
    "\n",
    "\n",
    "def get_mongo_client(**args):\n",
    "    '''Get MongoDB Client Connection'''\n",
    "    mongo_uri = get_mongo_uri(**args)\n",
    "    if args['cluster_location'] == \"atlas\":\n",
    "        client = pymongo.MongoClient(mongo_uri, tlsCAFile=certifi.where())\n",
    "\n",
    "    elif args['cluster_location'] == \"local\":\n",
    "        client = pymongo.MongoClient(mongo_uri)\n",
    "        \n",
    "    else:\n",
    "        raise Exception(\"A MongoDB Client could not be created.\")\n",
    "\n",
    "    return client\n",
    "    \n",
    "    \n",
    "# TODO: Rewrite this to leverage PySpark?\n",
    "def set_mongo_collections(mongo_client, db_name : str, data_directory : str, json_files : list):\n",
    "    db = mongo_client[db_name]\n",
    "    \n",
    "    for file in json_files:\n",
    "        db.drop_collection(file)\n",
    "        json_file = os.path.join(data_directory, json_files[file])\n",
    "        with open(json_file, 'r') as openfile:\n",
    "            json_object = json.load(openfile)\n",
    "            file = db[file]\n",
    "            result = file.insert_many(json_object)\n",
    "        \n",
    "    mongo_client.close()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def get_mongodb_dataframe(spark_session, **args):\n",
    "    '''Query MongoDB, and create a DataFrame'''\n",
    "    dframe = spark_session.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"database\", args['db_name']) \\\n",
    "        .option(\"collection\", args['collection']).load()\n",
    "\n",
    "    '''Drop the '_id' index column to clean up the response.'''\n",
    "    dframe = dframe.drop('_id')\n",
    "    \n",
    "    '''Call the drop_null_columns() function passing in the dataframe.'''\n",
    "    dframe = drop_null_columns(dframe, args['null_column_threshold'])\n",
    "    \n",
    "    return dframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a9db9-2354-4015-af02-0fd7ee8d8100",
   "metadata": {},
   "source": [
    "### 4.0. Initialize Data Lakehouse Directory Structure\n",
    "Remove the Data Lakehouse Database Directory Structure to Ensure Idempotency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a61b61e-09ca-40e8-a9ea-a102608a75af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Directory 'C:\\\\Users\\\\stell\\\\OneDrive\\\\Desktop\\\\Documents\\\\DS-2002-main\\\\04-PySpark\\\\spark-warehouse\\\\estella_dw.db' has been removed successfully.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "database_dir = os.path.join(sql_warehouse_dir, \"estella_dw.db\")\n",
    "remove_directory_tree(database_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1450e164-4cc4-40f0-ad0c-bead094c3a72",
   "metadata": {},
   "source": [
    "### 5.0. Create a New Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9010b44a-be91-422b-a179-a26166e691a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://HPPav04.localdomain:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[6]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark Estella Data Lakehouse (Medallion Architecture)</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x261dc9c1280>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "worker_threads = f\"local[{int(os.cpu_count()/2)}]\"\n",
    "\n",
    "jars = []\n",
    "mysql_spark_jar = os.path.join(os.getcwd(), \"mysql-connector-j-9.1.0\", \"mysql-connector-j-9.1.0.jar\")\n",
    "mssql_spark_jar = os.path.join(os.getcwd(), \"sqljdbc_12.8\", \"enu\", \"jars\", \"mssql-jdbc-12.8.1.jre11.jar\")\n",
    "\n",
    "jars.append(mysql_spark_jar)\n",
    "#jars.append(mssql_spark_jar)\n",
    "\n",
    "sparkConf_args = get_spark_conf_args(jars, **mongodb_args)\n",
    "\n",
    "sparkConf = get_spark_conf(**sparkConf_args)\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n",
    "spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df505d77-2a3b-4ae3-93e4-b60048e46386",
   "metadata": {},
   "source": [
    "### 6.0. Create a New Metadata Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97abc16c-b403-4c23-8200-544d3ea05450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DROP DATABASE IF EXISTS {dest_database} CASCADE;\")\n",
    "\n",
    "sql_create_db = f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {dest_database}\n",
    "    COMMENT 'DS-2002 AdventureWorks Data Lakehouse'\n",
    "    WITH DBPROPERTIES (contains_pii = true, purpose = 'DS-2002 Final Project');\n",
    "\"\"\"\n",
    "spark.sql(sql_create_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1dcd64-e074-4a1a-a9c7-8e5165e7a659",
   "metadata": {},
   "source": [
    "## Section II: Populate Dimensions by Ingesting \"Cold-path\" Reference Data \n",
    "### 1.0. Fetch Data from the File System\n",
    "#### 1.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ece0062-386d-40fa-95ba-c847514dc162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dim_customer.csv</td>\n",
       "      <td>916342</td>\n",
       "      <td>2025-12-17 19:18:00.996943474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dim_date.csv</td>\n",
       "      <td>64182</td>\n",
       "      <td>2025-12-17 19:21:36.731117249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dim_product.csv</td>\n",
       "      <td>12368</td>\n",
       "      <td>2025-12-17 19:22:34.330742836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dim_shipping_region.csv</td>\n",
       "      <td>113</td>\n",
       "      <td>2025-12-17 19:23:08.750877619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name    size             modification_time\n",
       "0         dim_customer.csv  916342 2025-12-17 19:18:00.996943474\n",
       "1             dim_date.csv   64182 2025-12-17 19:21:36.731117249\n",
       "2          dim_product.csv   12368 2025-12-17 19:22:34.330742836\n",
       "3  dim_shipping_region.csv     113 2025-12-17 19:23:08.750877619"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(batch_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b475c72-63e0-4e9f-9d84-db7223a9dd53",
   "metadata": {},
   "source": [
    "#### 1.2. Populate the <span style=\"color:darkred\">Customer Dimension</span>\n",
    "##### 1.2.1. Use PySpark to Read data from a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b59aaa23-b188-4140-8448-eb4c7ffd1a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\batch\\dim_customer.csv\n",
      "+---------------+-----------+---------+-------+-------+-------+\n",
      "|customer_id_key|customer_id|full_name|   city|  state|country|\n",
      "+---------------+-----------+---------+-------+-------+-------+\n",
      "|              1|          2|  Unknown|Unknown|Unknown|Unknown|\n",
      "|              2|          3|  Unknown|Unknown|Unknown|Unknown|\n",
      "|              3|          4|  Unknown|Unknown|Unknown|Unknown|\n",
      "|              4|          5|  Unknown|Unknown|Unknown|Unknown|\n",
      "|              5|          6|  Unknown|Unknown|Unknown|Unknown|\n",
      "+---------------+-----------+---------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Reading C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\batch\\dim_date.csv\n",
      "+------------+--------+----------+----+-----+----+-------+\n",
      "|date_key_key|date_key| full_date| day|month|year|quarter|\n",
      "+------------+--------+----------+----+-----+----+-------+\n",
      "|           1|20010702|2001-07-02|2001|    7|   2|      1|\n",
      "|           2|20010703|2001-07-03|2001|    7|   3|      2|\n",
      "|           3|20010704|2001-07-04|2001|    7|   4|      3|\n",
      "|           4|20010705|2001-07-05|2001|    7|   5|      4|\n",
      "|           5|20010706|2001-07-06|2001|    7|   6|      5|\n",
      "+------------+--------+----------+----+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Reading C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\batch\\dim_product.csv\n",
      "+--------------+----------+--------------------+--------+-----------+----------+\n",
      "|product_id_key|product_id|        product_name|category|subcategory|list_price|\n",
      "+--------------+----------+--------------------+--------+-----------+----------+\n",
      "|             1|       708|Sport-100 Helmet,...|   Black|      34.99|      NULL|\n",
      "|             2|       709|Mountain Bike Soc...|   White|        9.5|      NULL|\n",
      "|             3|       710|Mountain Bike Soc...|   White|        9.5|      NULL|\n",
      "|             4|       711|Sport-100 Helmet,...|    Blue|      34.99|      NULL|\n",
      "|             5|       712|        AWC Logo Cap|   Multi|       8.99|      NULL|\n",
      "+--------------+----------+--------------------+--------+-----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Reading C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\batch\\dim_shipping_region.csv\n",
      "+-------------+---------+-------------+\n",
      "|region_id_key|region_id|  region_name|\n",
      "+-------------+---------+-------------+\n",
      "|            1|        2|   West Coast|\n",
      "|            2|        3|      Midwest|\n",
      "|            3|        4|        South|\n",
      "|            4|        5|International|\n",
      "+-------------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DateType, DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "import os\n",
    "\n",
    "batch_dir = r\"C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\batch\"\n",
    "\n",
    "files_and_schemas = {\n",
    "    \"dim_customer.csv\": StructType([\n",
    "        StructField(\"customer_id\", IntegerType(), True),\n",
    "        StructField(\"full_name\", StringType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True),\n",
    "        StructField(\"country\", StringType(), True)\n",
    "    ]),\n",
    "    \"dim_date.csv\": StructType([\n",
    "        StructField(\"date_key\", IntegerType(), True),\n",
    "        StructField(\"full_date\", DateType(), True),\n",
    "        StructField(\"day\", IntegerType(), True),\n",
    "        StructField(\"month\", IntegerType(), True),\n",
    "        StructField(\"year\", IntegerType(), True),\n",
    "        StructField(\"quarter\", IntegerType(), True)\n",
    "    ]),\n",
    "    \"dim_product.csv\": StructType([\n",
    "        StructField(\"product_id\", IntegerType(), True),\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"category\", StringType(), True),\n",
    "        StructField(\"subcategory\", StringType(), True),\n",
    "        StructField(\"list_price\", DoubleType(), True)\n",
    "    ]),\n",
    "    \"dim_shipping_region.csv\": StructType([\n",
    "        StructField(\"region_id\", IntegerType(), True),\n",
    "        StructField(\"region_name\", StringType(), True)\n",
    "    ])\n",
    "}\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "df_dimensions = {} \n",
    "\n",
    "for file_name, schema in files_and_schemas.items():\n",
    "    file_path = os.path.join(batch_dir, file_name)\n",
    "    print(f\"Reading {file_path}\")\n",
    "    \n",
    "    df = spark.read.csv(file_path, header=True, schema=schema)\n",
    "    \n",
    "   \n",
    "    primary_col = schema.fields[0].name  \n",
    "    window_spec = Window.orderBy(primary_col)\n",
    "    df = df.withColumn(f\"{primary_col}_key\", row_number().over(window_spec))\n",
    "    \n",
    "\n",
    "    cols = [f\"{primary_col}_key\"] + [f.name for f in schema.fields]\n",
    "    df = df.select(*cols)\n",
    "    \n",
    "    df.show(5)\n",
    "    \n",
    "    df_dimensions[file_name] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ba33af4-f2c2-4255-905a-9c2b9074ea53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+---------+-------+-------+-------+\n",
      "|customer_id_key|customer_id|full_name|   city|  state|country|\n",
      "+---------------+-----------+---------+-------+-------+-------+\n",
      "|              1|          2|  Unknown|Unknown|Unknown|Unknown|\n",
      "|              2|          3|  Unknown|Unknown|Unknown|Unknown|\n",
      "|              3|          4|  Unknown|Unknown|Unknown|Unknown|\n",
      "|              4|          5|  Unknown|Unknown|Unknown|Unknown|\n",
      "|              5|          6|  Unknown|Unknown|Unknown|Unknown|\n",
      "+---------------+-----------+---------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_dim_customer = df_dimensions[\"dim_customer.csv\"]\n",
    "df_dim_customer.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a1a30f-55ac-44cf-80b2-cebd1cfe7bee",
   "metadata": {},
   "source": [
    "##### 1.2.2. Make Necessary Transformations to the New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d8556e8-9ef3-48e3-bda2-0abf5813c82a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id_orig</th>\n",
       "      <th>full_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id_orig full_name     city    state  country\n",
       "0             1                 2   Unknown  Unknown  Unknown  Unknown\n",
       "1             2                 3   Unknown  Unknown  Unknown  Unknown\n",
       "2             3                 4   Unknown  Unknown  Unknown  Unknown\n",
       "3             4                 5   Unknown  Unknown  Unknown  Unknown\n",
       "4             5                 6   Unknown  Unknown  Unknown  Unknown"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dim_customer = df_dim_customer.withColumnRenamed(\"customer_id\", \"customer_id_orig\")  \n",
    "\n",
    "df_dim_customer.createOrReplaceTempView(\"customers\")\n",
    "sql_customers = \"\"\"\n",
    "    SELECT *, ROW_NUMBER() OVER (ORDER BY customer_id_orig) AS customer_key\n",
    "    FROM customers\n",
    "\"\"\"\n",
    "df_dim_customer = spark.sql(sql_customers)\n",
    "\n",
    "ordered_columns = ['customer_key', 'customer_id_orig', 'full_name', 'city', 'state', 'country']\n",
    "df_dim_customer = df_dim_customer[ordered_columns]\n",
    "\n",
    "df_dim_customer.limit(5).toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f7ea0e-acaa-4dc5-b021-8f2923fe49c2",
   "metadata": {},
   "source": [
    "##### 1.2.3. Save as the <span style=\"color:darkred\">dim_customer</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "351a9a57-af47-4cdf-a4fa-4e4eaddbf918",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customer.write.saveAsTable(f\"{dest_database}.dim_customer\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accf0265-7be2-4cc1-8706-9fc7c89bc3f7",
   "metadata": {},
   "source": [
    "##### 1.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "586eed71-e804-4425-a837-aec8add107ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|        customer_key|                 int|   NULL|\n",
      "|    customer_id_orig|                 int|   NULL|\n",
      "|           full_name|              string|   NULL|\n",
      "|                city|              string|   NULL|\n",
      "|               state|              string|   NULL|\n",
      "|             country|              string|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|          estella_dw|       |\n",
      "|               Table|        dim_customer|       |\n",
      "|        Created Time|Thu Dec 18 00:21:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.4|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/C:/Users/st...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_key</th>\n",
       "      <th>customer_id_orig</th>\n",
       "      <th>full_name</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customer_key  customer_id_orig full_name     city    state  country\n",
       "0             1                 2   Unknown  Unknown  Unknown  Unknown\n",
       "1             2                 3   Unknown  Unknown  Unknown  Unknown"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_customer\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_customer LIMIT 2\").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2604df8a-d9d0-4ab9-b826-416db4ea64b3",
   "metadata": {},
   "source": [
    "#### 1.3. Populate the <span style=\"color:darkred\">Date Dimension</span>\n",
    "##### 1.3.1. Use PySpark to Read Data from a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7fdd5e7-6b27-4e78-be0a-d728138d38ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----+---+---+---+---+\n",
      "|     _c0|                _c1| _c2|_c3|_c4|_c5|_c6|\n",
      "+--------+-------------------+----+---+---+---+---+\n",
      "|20010701|2001-07-01 00:00:00|2001|  7|  1|  7|  1|\n",
      "|20010702|2001-07-02 00:00:00|2001|  7|  2|  1|  0|\n",
      "|20010703|2001-07-03 00:00:00|2001|  7|  3|  2|  0|\n",
      "|20010704|2001-07-04 00:00:00|2001|  7|  4|  3|  0|\n",
      "|20010705|2001-07-05 00:00:00|2001|  7|  5|  4|  0|\n",
      "+--------+-------------------+----+---+---+---+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------------+--------+----------+---+-----+----+-------+\n",
      "|date_key_surrogate|date_key| full_date|day|month|year|quarter|\n",
      "+------------------+--------+----------+---+-----+----+-------+\n",
      "|                 1|20010701|2001-07-01|  1|    1|2001|      1|\n",
      "|                 2|20010702|2001-07-02|  2|    0|2001|      0|\n",
      "|                 3|20010703|2001-07-03|  3|    0|2001|      0|\n",
      "|                 4|20010704|2001-07-04|  4|    0|2001|      0|\n",
      "|                 5|20010705|2001-07-05|  5|    0|2001|      0|\n",
      "+------------------+--------+----------+---+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, row_number\n",
    "from pyspark.sql.window import Window\n",
    "import os\n",
    "\n",
    "df_dim_date_raw = spark.read.csv(\n",
    "    os.path.join(batch_dir, \"dim_date.csv\"),\n",
    "    header=False,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "df_dim_date_raw.show(5)\n",
    "\n",
    "df_dim_date = (\n",
    "    df_dim_date_raw\n",
    "    .select(\n",
    "        col(\"_c0\").cast(\"int\").alias(\"date_key\"),\n",
    "        col(\"_c1\").cast(\"date\").alias(\"full_date\"),\n",
    "        col(\"_c2\").cast(\"int\").alias(\"year\"),\n",
    "        col(\"_c3\").cast(\"int\").alias(\"day_of_year\"),\n",
    "        col(\"_c4\").cast(\"int\").alias(\"day\"),\n",
    "        col(\"_c5\").cast(\"int\").alias(\"week\"),\n",
    "        col(\"_c6\").cast(\"int\").alias(\"month\")\n",
    "    )\n",
    ")\n",
    "from pyspark.sql.functions import ceil, col\n",
    "\n",
    "df_dim_date = df_dim_date.withColumn(\n",
    "    \"quarter\",\n",
    "    ceil(col(\"month\") / 3)\n",
    ")\n",
    "\n",
    "window_spec = Window.orderBy(\"date_key\")\n",
    "\n",
    "df_dim_date = (\n",
    "    df_dim_date\n",
    "    .withColumn(\"date_key_surrogate\", row_number().over(window_spec))\n",
    "    .select(\n",
    "        \"date_key_surrogate\",\n",
    "        \"date_key\",\n",
    "        \"full_date\",\n",
    "        \"day\",\n",
    "        \"month\",\n",
    "        \"year\",\n",
    "        \"quarter\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_dim_date.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be69180-2574-4bd2-a8c0-6933c56fe50f",
   "metadata": {},
   "source": [
    "##### 1.3.2 Make Necessary Transformations to the New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb661192-c10f-4b9d-a913-4e0cfa8050c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------+----------+---+-----+----+-------+\n",
      "|date_key_surrogate|date_key| full_date|day|month|year|quarter|\n",
      "+------------------+--------+----------+---+-----+----+-------+\n",
      "|                 1|20010701|2001-07-01|  1|    1|2001|      1|\n",
      "|                 2|20010702|2001-07-02|  2|    0|2001|      0|\n",
      "|                 3|20010703|2001-07-03|  3|    0|2001|      0|\n",
      "|                 4|20010704|2001-07-04|  4|    0|2001|      0|\n",
      "|                 5|20010705|2001-07-05|  5|    0|2001|      0|\n",
      "+------------------+--------+----------+---+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.orderBy(\"date_key\")\n",
    "\n",
    "df_dim_date = (\n",
    "    df_dim_date\n",
    "    .withColumn(\"date_key_surrogate\", row_number().over(window_spec))\n",
    "    .select(\n",
    "        \"date_key_surrogate\",\n",
    "        \"date_key\",\n",
    "        \"full_date\",\n",
    "        \"day\",\n",
    "        \"month\",\n",
    "        \"year\",\n",
    "        \"quarter\"\n",
    "    )\n",
    ")\n",
    "\n",
    "df_dim_date.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c75568-3dca-4a67-9b03-35049dfb2abf",
   "metadata": {},
   "source": [
    "##### 1.3.3. Save as the <span style=\"color:darkred\">dim_date</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76dc539a-35b1-4d8b-94c0-243a094c74f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_date.write.saveAsTable(f\"{dest_database}.dim_date\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0590f931-c005-49c6-86e4-9e5a951c44f1",
   "metadata": {},
   "source": [
    "##### 1.3.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7850fbb-1b23-4d3f-9b06-622bcbb3c877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|  date_key_surrogate|                 int|   NULL|\n",
      "|            date_key|                 int|   NULL|\n",
      "|           full_date|                date|   NULL|\n",
      "|                 day|                 int|   NULL|\n",
      "|               month|                 int|   NULL|\n",
      "|                year|                 int|   NULL|\n",
      "|             quarter|              bigint|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|          estella_dw|       |\n",
      "|               Table|            dim_date|       |\n",
      "|        Created Time|Thu Dec 18 00:22:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.4|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/C:/Users/st...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key_surrogate</th>\n",
       "      <th>date_key</th>\n",
       "      <th>full_date</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20010701</td>\n",
       "      <td>2001-07-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20010702</td>\n",
       "      <td>2001-07-02</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   date_key_surrogate  date_key   full_date  day  month  year  quarter\n",
       "0                   1  20010701  2001-07-01    1      1  2001        1\n",
       "1                   2  20010702  2001-07-02    2      0  2001        0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_date\").show()\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_date LIMIT 2\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dbfe61-ec94-4136-9d05-6eb0e953dd8c",
   "metadata": {},
   "source": [
    "#### 2.0. Fetch Reference Data from a MongoDB Atlas Database\n",
    "#### 2.1. Create a New MongoDB Database, and Load Each JSON File into a New MongoDB Collection\n",
    "**NOTE:** The following cell **can** be run more than once because the **set_mongo_collection()** function **is** idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3035e912-c6ad-4e70-a52b-8c400fb4a497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 265 records into MongoDB collection 'dim_product'\n",
      "Loaded 4 records into MongoDB collection 'dim_shipping_region'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_files = {\n",
    "    \"dim_product\": \"dim_product.csv\",\n",
    "    \"dim_shipping_region\": \"dim_shipping_region.csv\"\n",
    "}\n",
    "\n",
    "\n",
    "client = get_mongo_client(**mongodb_args)\n",
    "db = client[mongodb_args[\"db_name\"]]\n",
    "\n",
    "\n",
    "for collection_name, csv_file in csv_files.items():\n",
    "    csv_path = os.path.join(batch_dir, csv_file)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "    \n",
    "    \n",
    "    db.drop_collection(collection_name)\n",
    "    db[collection_name].insert_many(records)\n",
    "    print(f\"Loaded {len(records)} records into MongoDB collection '{collection_name}'\")\n",
    "\n",
    "\n",
    "client.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b68b610-5970-4a13-a193-4410dfcf3b43",
   "metadata": {},
   "source": [
    "#### 2.2. Populate the <span style=\"color:darkred\">Product</span>\n",
    "##### 2.2.1. Fetch Data from the New MongoDB <span style=\"color:darkred\">Product</span> Collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7a0f512-1670-44e6-aeae-b9708d9f952e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>34.99</th>\n",
       "      <th>707</th>\n",
       "      <th>Red</th>\n",
       "      <th>Sport-100 Helmet, Red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>708</td>\n",
       "      <td>Black</td>\n",
       "      <td>Sport-100 Helmet, Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>709</td>\n",
       "      <td>White</td>\n",
       "      <td>Mountain Bike Socks, M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>710</td>\n",
       "      <td>White</td>\n",
       "      <td>Mountain Bike Socks, L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>711</td>\n",
       "      <td>Blue</td>\n",
       "      <td>Sport-100 Helmet, Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>712</td>\n",
       "      <td>Multi</td>\n",
       "      <td>AWC Logo Cap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   34.99  707    Red    Sport-100 Helmet, Red\n",
       "0    NaN  708  Black  Sport-100 Helmet, Black\n",
       "1    NaN  709  White   Mountain Bike Socks, M\n",
       "2    NaN  710  White   Mountain Bike Socks, L\n",
       "3    NaN  711   Blue   Sport-100 Helmet, Blue\n",
       "4    NaN  712  Multi             AWC Logo Cap"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "mongodb_args[\"collection\"] = \"dim_product\"\n",
    "\n",
    "\n",
    "df_dim_product = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "    .option(\"database\", mongodb_args['db_name']) \\\n",
    "    .option(\"collection\", mongodb_args['collection']).load()\n",
    "\n",
    "if \"_id\" in df_dim_product.columns:\n",
    "    df_dim_product = df_dim_product.drop(\"_id\")\n",
    "\n",
    "df_dim_product.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec11cf8a-35e9-41da-9c2d-fb7cee0dd7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-----+--------------------+\n",
      "|product_id|product_number|color|                name|\n",
      "+----------+--------------+-----+--------------------+\n",
      "|      NULL|           708|Black|Sport-100 Helmet,...|\n",
      "|      NULL|           709|White|Mountain Bike Soc...|\n",
      "|      NULL|           710|White|Mountain Bike Soc...|\n",
      "|      NULL|           711| Blue|Sport-100 Helmet,...|\n",
      "|      NULL|           712|Multi|        AWC Logo Cap|\n",
      "+----------+--------------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "correct_columns = [\n",
    "    \"product_id\", \"product_number\", \"color\", \"name\"\n",
    "]\n",
    "df_dim_product = df_dim_product.toDF(*correct_columns)\n",
    "\n",
    "df_dim_product.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d2f7e50-4683-4302-8371-3bbb84529106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_product.write.saveAsTable(f\"{dest_database}.dim_product\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd62655-61c9-4e09-a1d4-75e5dd53f41f",
   "metadata": {},
   "source": [
    "##### 2.2.2. Make Necessary Transformations to the New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a289d292-9ba5-41c4-9496-4c08a9514ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_number</th>\n",
       "      <th>name</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>708</td>\n",
       "      <td>Sport-100 Helmet, Black</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>709</td>\n",
       "      <td>Mountain Bike Socks, M</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>710</td>\n",
       "      <td>Mountain Bike Socks, L</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>711</td>\n",
       "      <td>Sport-100 Helmet, Blue</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>712</td>\n",
       "      <td>AWC Logo Cap</td>\n",
       "      <td>Multi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  product_id  product_number                     name  color\n",
       "0            1         NaN             708  Sport-100 Helmet, Black  Black\n",
       "1            2         NaN             709   Mountain Bike Socks, M  White\n",
       "2            3         NaN             710   Mountain Bike Socks, L  White\n",
       "3            4         NaN             711   Sport-100 Helmet, Blue   Blue\n",
       "4            5         NaN             712             AWC Logo Cap  Multi"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.orderBy(\"product_id\")\n",
    "df_dim_product = df_dim_product.withColumn(\"product_key\", row_number().over(window_spec))\n",
    "\n",
    "df_dim_product_clean = df_dim_product.select(\n",
    "    df_dim_product[\"product_key\"],\n",
    "    df_dim_product[\"product_id\"],\n",
    "    df_dim_product[\"product_number\"],\n",
    "    df_dim_product[\"name\"],\n",
    "    df_dim_product[\"color\"]\n",
    "  \n",
    ")\n",
    "\n",
    "df_dim_product_clean.limit(5).toPandas()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e050538-ff58-4187-a031-9a3b2ac522dd",
   "metadata": {},
   "source": [
    "##### 2.2.3. Save as the <span style=\"color:darkred\">dim_product</span> table in the Data lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "65520d80-027e-4422-beb4-7f2da0ad5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dim_product_clean.write.saveAsTable(f\"{dest_database}.dim_product\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533d5c4c-f792-42f0-b42c-74ac672053cf",
   "metadata": {},
   "source": [
    "##### 2.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8af6d5a-c6d7-4374-9a4c-0ad7fb0af201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+\n",
      "| namespace|   tableName|isTemporary|\n",
      "+----------+------------+-----------+\n",
      "|estella_dw|dim_customer|      false|\n",
      "|estella_dw|    dim_date|      false|\n",
      "|estella_dw| dim_product|      false|\n",
      "|          |   customers|       true|\n",
      "+----------+------------+-----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_key</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_number</th>\n",
       "      <th>name</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>708</td>\n",
       "      <td>Sport-100 Helmet, Black</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>709</td>\n",
       "      <td>Mountain Bike Socks, M</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>710</td>\n",
       "      <td>Mountain Bike Socks, L</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>711</td>\n",
       "      <td>Sport-100 Helmet, Blue</td>\n",
       "      <td>Blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>712</td>\n",
       "      <td>AWC Logo Cap</td>\n",
       "      <td>Multi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_key  product_id  product_number                     name  color\n",
       "0            1         NaN             708  Sport-100 Helmet, Black  Black\n",
       "1            2         NaN             709   Mountain Bike Socks, M  White\n",
       "2            3         NaN             710   Mountain Bike Socks, L  White\n",
       "3            4         NaN             711   Sport-100 Helmet, Blue   Blue\n",
       "4            5         NaN             712             AWC Logo Cap  Multi"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(f\"SHOW TABLES IN {dest_database}\").show()\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_product LIMIT 5\").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab5614-4874-4940-9d6f-98f7638760cd",
   "metadata": {},
   "source": [
    "#### 2.4. Populate the <span style=\"color:darkred\">Shipping Regions</span>\n",
    "##### 2.3.1. Fetch Data from the New MongoDB <span style=\"color:darkred\">Shipping Regions</span> Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9212664d-9f79-47c1-8739-aa810bd9f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mongodb_args[\"collection\"] = \"dim_shipping_region\"\n",
    "\n",
    "df_dim_shipping_region = get_mongodb_dataframe(spark, **mongodb_args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1ce6e6-3dc0-4c9b-939c-e9a2807115a1",
   "metadata": {},
   "source": [
    "##### 2.3.2. Make Necessary Transformations to the New Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2e9b5bc9-ba67-426f-bf49-e28fe065cc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shippingregion_key</th>\n",
       "      <th>shippingregion_id</th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>West Coast</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>South</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>International</td>\n",
       "      <td>Global</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shippingregion_key  shippingregion_id           name country\n",
       "0                   1                  2     West Coast     USA\n",
       "1                   2                  3        Midwest     USA\n",
       "2                   3                  4          South     USA\n",
       "3                   4                  5  International  Global"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "df_dim_shipping_region = df_dim_shipping_region.toDF(\n",
    "    \"shippingregion_id\",  \n",
    "    \"name\",               \n",
    "    \"country\"            \n",
    ")\n",
    "\n",
    "\n",
    "window_spec = Window.orderBy(\"shippingregion_id\")\n",
    "df_dim_shipping_region = df_dim_shipping_region.withColumn(\"shippingregion_key\", row_number().over(window_spec))\n",
    "\n",
    "\n",
    "df_dim_shipping_region_clean = df_dim_shipping_region.select(\n",
    "    \"shippingregion_key\",\n",
    "    \"shippingregion_id\",\n",
    "    \"name\",\n",
    "    \"country\"\n",
    ")\n",
    "\n",
    "\n",
    "df_dim_shipping_region_clean.limit(5).toPandas()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c228d196-ff92-4a2c-9347-fe4b1f6a4a74",
   "metadata": {},
   "source": [
    "##### 2.3.3. Save as the <span style=\"color:darkred\">dim_shipping_regions</span> table in the Data lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff72a143-bb70-403a-a75a-6e5d2b8ae08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dim_shipping_region_clean.write.saveAsTable(\n",
    "    f\"{dest_database}.dim_shipping_region\",\n",
    "    mode=\"overwrite\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c4a5c7-829e-4a33-a58f-4bb7c021afb9",
   "metadata": {},
   "source": [
    "##### 2.3.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f299a3ab-a358-48f8-aa22-b2518a8f5112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+-------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                                                |comment|\n",
      "+----------------------------+-------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|shippingregion_key          |int                                                                                                                      |NULL   |\n",
      "|shippingregion_id           |int                                                                                                                      |NULL   |\n",
      "|name                        |string                                                                                                                   |NULL   |\n",
      "|country                     |string                                                                                                                   |NULL   |\n",
      "|                            |                                                                                                                         |       |\n",
      "|# Detailed Table Information|                                                                                                                         |       |\n",
      "|Catalog                     |spark_catalog                                                                                                            |       |\n",
      "|Database                    |estella_dw                                                                                                               |       |\n",
      "|Table                       |dim_shipping_region                                                                                                      |       |\n",
      "|Created Time                |Thu Dec 18 00:22:21 EST 2025                                                                                             |       |\n",
      "|Last Access                 |UNKNOWN                                                                                                                  |       |\n",
      "|Created By                  |Spark 3.5.4                                                                                                              |       |\n",
      "|Type                        |MANAGED                                                                                                                  |       |\n",
      "|Provider                    |parquet                                                                                                                  |       |\n",
      "|Location                    |file:/C:/Users/stell/OneDrive/Desktop/Documents/DS-2002-main/04-PySpark/spark-warehouse/estella_dw.db/dim_shipping_region|       |\n",
      "+----------------------------+-------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>shippingregion_key</th>\n",
       "      <th>shippingregion_id</th>\n",
       "      <th>name</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>West Coast</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Midwest</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>South</td>\n",
       "      <td>USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>International</td>\n",
       "      <td>Global</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   shippingregion_key  shippingregion_id           name country\n",
       "0                   1                  2     West Coast     USA\n",
       "1                   2                  3        Midwest     USA\n",
       "2                   3                  4          South     USA\n",
       "3                   4                  5  International  Global"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_shipping_region\").show(truncate=False)\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_shipping_region LIMIT 5\").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466ad9d8-046e-41a4-970a-56ff12c52eb7",
   "metadata": {},
   "source": [
    "### 3.0. Fetch Reference Data from a MySQL Database\n",
    "#### 3.1. Populate the <span style=\"color:darkred\">Currency Dimension</span>\n",
    "##### 3.1.1 Fetch data from the <span style=\"color:darkred\">dim_currency</span> table in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6b342ca-6a22-4022-91a8-c287de19252d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CurrencyCode</th>\n",
       "      <th>Name</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AED</td>\n",
       "      <td>Emirati Dirham</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFA</td>\n",
       "      <td>Afghani</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALL</td>\n",
       "      <td>Lek</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMD</td>\n",
       "      <td>Armenian Dram</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANG</td>\n",
       "      <td>Netherlands Antillian Guilder</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CurrencyCode                           Name ModifiedDate\n",
       "0          AED                 Emirati Dirham   1998-06-01\n",
       "1          AFA                        Afghani   1998-06-01\n",
       "2          ALL                            Lek   1998-06-01\n",
       "3          AMD                  Armenian Dram   1998-06-01\n",
       "4          ANG  Netherlands Antillian Guilder   1998-06-01"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sql_dim_currency = f\"SELECT * FROM {mysql_args['db_name']}.dim_currency\"\n",
    "\n",
    "df_dim_currency = get_mysql_dataframe(spark, sql_dim_currency, **mysql_args)\n",
    "\n",
    "df_dim_currency.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ee9c87-afaf-4d56-9910-78e3dd281fa8",
   "metadata": {},
   "source": [
    "##### 3.1.2. Save as the <span style=\"color:darkred\">dim_date</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fbae01c5-b7a6-4ab3-86a0-10137c492444",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_currency.write.saveAsTable(f\"{dest_database}.dim_currency\", mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c47041-b350-4bc5-a8a2-eb1320d1f74c",
   "metadata": {},
   "source": [
    "##### 3.1.3. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "acbbcac9-fc42-4fc8-9ad5-d3ea9c897e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|        CurrencyCode|          varchar(3)|   NULL|\n",
      "|                Name|         varchar(50)|   NULL|\n",
      "|        ModifiedDate|           timestamp|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|          estella_dw|       |\n",
      "|               Table|        dim_currency|       |\n",
      "|        Created Time|Thu Dec 18 00:22:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.4|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/C:/Users/st...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CurrencyCode</th>\n",
       "      <th>Name</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AED</td>\n",
       "      <td>Emirati Dirham</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFA</td>\n",
       "      <td>Afghani</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALL</td>\n",
       "      <td>Lek</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMD</td>\n",
       "      <td>Armenian Dram</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANG</td>\n",
       "      <td>Netherlands Antillian Guilder</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CurrencyCode                           Name ModifiedDate\n",
       "0          AED                 Emirati Dirham   1998-06-01\n",
       "1          AFA                        Afghani   1998-06-01\n",
       "2          ALL                            Lek   1998-06-01\n",
       "3          AMD                  Armenian Dram   1998-06-01\n",
       "4          ANG  Netherlands Antillian Guilder   1998-06-01"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_currency\").show()\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_currency LIMIT 5\").toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f7074-cfb6-4384-8549-114a76be7701",
   "metadata": {},
   "source": [
    "#### 3.2. Populate the <span style=\"color:darkred\">Product Dimension</span>\n",
    "##### 3.2.1. Fetch data from the <span style=\"color:darkred\">Products</span> table in MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f488c45e-8323-4d90-a2e6-45f1913b4749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CultureID</th>\n",
       "      <th>Name</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>Invariant Language (Invariant Country)</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ar</td>\n",
       "      <td>Arabic</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>es</td>\n",
       "      <td>Spanish</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fr</td>\n",
       "      <td>French</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CultureID                                    Name ModifiedDate\n",
       "0            Invariant Language (Invariant Country)   1998-06-01\n",
       "1    ar                                      Arabic   1998-06-01\n",
       "2    en                                     English   1998-06-01\n",
       "3    es                                     Spanish   1998-06-01\n",
       "4    fr                                      French   1998-06-01"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_dim_culture = f\"SELECT * FROM {mysql_args['db_name']}.dim_culture\"\n",
    "\n",
    "df_dim_culture = get_mysql_dataframe(spark, sql_dim_culture, **mysql_args)\n",
    "\n",
    "df_dim_culture.limit(5).toPandas()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec943028-59ed-46a7-9789-5dc5910a9139",
   "metadata": {},
   "source": [
    "##### 3.2.3. Save as the <span style=\"color:darkred\">dim_products</span> table in the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a87c7bf-5e28-4def-abbb-f1722c008b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_currency.write.saveAsTable(f\"{dest_database}.dim_culture\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47356302-2197-48b2-a9c4-50ea3369a7f4",
   "metadata": {},
   "source": [
    "##### 3.2.4. Unit Test: Describe and Preview Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3000ae69-63da-4cb3-8cdc-469de660797f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|        CurrencyCode|          varchar(3)|   NULL|\n",
      "|                Name|         varchar(50)|   NULL|\n",
      "|        ModifiedDate|           timestamp|   NULL|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|             Catalog|       spark_catalog|       |\n",
      "|            Database|          estella_dw|       |\n",
      "|               Table|         dim_culture|       |\n",
      "|        Created Time|Thu Dec 18 00:22:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.5.4|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/C:/Users/st...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CurrencyCode</th>\n",
       "      <th>Name</th>\n",
       "      <th>ModifiedDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AED</td>\n",
       "      <td>Emirati Dirham</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFA</td>\n",
       "      <td>Afghani</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALL</td>\n",
       "      <td>Lek</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMD</td>\n",
       "      <td>Armenian Dram</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANG</td>\n",
       "      <td>Netherlands Antillian Guilder</td>\n",
       "      <td>1998-06-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CurrencyCode                           Name ModifiedDate\n",
       "0          AED                 Emirati Dirham   1998-06-01\n",
       "1          AFA                        Afghani   1998-06-01\n",
       "2          ALL                            Lek   1998-06-01\n",
       "3          AMD                  Armenian Dram   1998-06-01\n",
       "4          ANG  Netherlands Antillian Guilder   1998-06-01"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"DESCRIBE EXTENDED {dest_database}.dim_culture\").show()\n",
    "spark.sql(f\"SELECT * FROM {dest_database}.dim_culture LIMIT 5\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60855fe-5282-4247-8186-9e835488bbce",
   "metadata": {},
   "source": [
    "### 4.0. Verify Dimension Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "520f3cae-b1a5-4d32-85aa-a07cf5eae560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>estella_dw</td>\n",
       "      <td>dim_culture</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>estella_dw</td>\n",
       "      <td>dim_currency</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>estella_dw</td>\n",
       "      <td>dim_customer</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>estella_dw</td>\n",
       "      <td>dim_date</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>estella_dw</td>\n",
       "      <td>dim_product</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>estella_dw</td>\n",
       "      <td>dim_shipping_region</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>customers</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    namespace            tableName  isTemporary\n",
       "0  estella_dw          dim_culture        False\n",
       "1  estella_dw         dim_currency        False\n",
       "2  estella_dw         dim_customer        False\n",
       "3  estella_dw             dim_date        False\n",
       "4  estella_dw          dim_product        False\n",
       "5  estella_dw  dim_shipping_region        False\n",
       "6                        customers         True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"USE {dest_database};\")\n",
    "spark.sql(\"SHOW TABLES\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98274ca9-111a-4de4-929f-a4dd65b744e6",
   "metadata": {},
   "source": [
    "## Section III: Integrate Reference Data with Real-Time Data\n",
    "### 6.0. Use PySpark Structured Streaming to Process (Hot Path) <span style=\"color:darkred\">Fact Sales</span> Fact Data  \n",
    "#### 6.1. Verify the location of the source data files on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0639c01e-49cf-4b3a-93f6-0cb8def88d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>size</th>\n",
       "      <th>modification_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fact_sales_43659_54147.json</td>\n",
       "      <td>218346</td>\n",
       "      <td>2025-12-17 18:32:01.172175407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fact_sales_54148_64636.json</td>\n",
       "      <td>216391</td>\n",
       "      <td>2025-12-17 18:32:36.945561647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fact_sales_64637_75123.json</td>\n",
       "      <td>216599</td>\n",
       "      <td>2025-12-17 18:33:36.349854708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          name    size             modification_time\n",
       "0  fact_sales_43659_54147.json  218346 2025-12-17 18:32:01.172175407\n",
       "1  fact_sales_54148_64636.json  216391 2025-12-17 18:32:36.945561647\n",
       "2  fact_sales_64637_75123.json  216599 2025-12-17 18:33:36.349854708"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_file_info(fact_sales_stream_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a4ce4-9da2-4aaf-ab59-043c3ce65c49",
   "metadata": {},
   "source": [
    "#### 6.2. Create the Bronze Layer: Stage <span style=\"color:darkred\">Fact Sales table</span> Data\n",
    "##### 6.2.1. Read \"Raw\" JSON file data into a Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "98987325-587d-428a-8b26-9794ad44f932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_sales_stream_dir = r\"C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\streaming\\fact_sales\"\n",
    "fact_sales_output_bronze = r\"C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\streaming\\fact_sales\\Bronze\"\n",
    "\n",
    "df_fact_sales_bronze = (\n",
    "    spark.readStream\n",
    "    .option(\"schemaLocation\", fact_sales_output_bronze)\n",
    "    .option(\"maxFilesPerTrigger\", 1)     \n",
    "    .option(\"multiLine\", \"true\")          \n",
    "    .json(fact_sales_stream_dir)         \n",
    ")\n",
    "\n",
    "df_fact_sales_bronze.isStreaming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78624adc-e24b-4880-9cf2-745bb5fab324",
   "metadata": {},
   "source": [
    "##### 6.2.2. Write the Streaming Data to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92ac4d20-fb3b-4a32-bbfb-51a1d1322832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "import os\n",
    "\n",
    "fact_sales_checkpoint_bronze = os.path.join(fact_sales_output_bronze, '_checkpoint')\n",
    "\n",
    "fact_sales_bronze_query = (\n",
    "    df_fact_sales_bronze\n",
    "    .withColumn(\"receipt_time\", current_timestamp())  \n",
    "    .withColumn(\"source_file\", input_file_name())    \n",
    "    .writeStream\n",
    "    .format(\"parquet\")\n",
    "    .outputMode(\"append\")\n",
    "    .queryName(\"fact_sales_bronze\")\n",
    "    .trigger(availableNow=True)                      \n",
    "    .option(\"checkpointLocation\", fact_sales_checkpoint_bronze)\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .start(fact_sales_output_bronze)\n",
    ")\n",
    "\n",
    "fact_sales_bronze_query.isActive\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79efc20-d327-4c72-a3cf-5a3ebbfaa7c9",
   "metadata": {},
   "source": [
    "##### 6.2.3. Unit Test: Implement Query Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "714410db-7dce-4381-9c7a-1147a3271d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query ID: b67dbe89-9562-4776-866e-da325f8e3878\n",
      "Query Name: fact_sales_bronze\n",
      "Query Status: {'message': 'Initializing sources', 'isDataAvailable': False, 'isTriggerActive': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(f\"Query ID: {fact_sales_bronze_query.id}\")\n",
    "print(f\"Query Name: {fact_sales_bronze_query.name}\")\n",
    "print(f\"Query Status: {fact_sales_bronze_query.status}\")\n",
    "\n",
    "fact_sales_bronze_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ad41b2e-e8cc-4b48-8ca9-50512c1eb762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- date_key: long (nullable = true)\n",
      " |-- extended_amount: double (nullable = true)\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_line_id: long (nullable = true)\n",
      " |-- product_id: long (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- region_id: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- receipt_time: timestamp (nullable = false)\n",
      " |-- source_file: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(fact_sales_output_bronze).printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a61077d4-2d98-4bc7-978c-c1e4d507608c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Table: dim_customer ===\n",
      "Schema:\n",
      "root\n",
      " |-- customer_key: integer (nullable = true)\n",
      " |-- customer_id_orig: integer (nullable = true)\n",
      " |-- full_name: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "Sample rows:\n",
      "+------------+----------------+---------+-------+-------+-------+\n",
      "|customer_key|customer_id_orig|full_name|city   |state  |country|\n",
      "+------------+----------------+---------+-------+-------+-------+\n",
      "|1           |2               |Unknown  |Unknown|Unknown|Unknown|\n",
      "|2           |3               |Unknown  |Unknown|Unknown|Unknown|\n",
      "|3           |4               |Unknown  |Unknown|Unknown|Unknown|\n",
      "|4           |5               |Unknown  |Unknown|Unknown|Unknown|\n",
      "|5           |6               |Unknown  |Unknown|Unknown|Unknown|\n",
      "+------------+----------------+---------+-------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=== Table: dim_product ===\n",
      "Schema:\n",
      "root\n",
      " |-- product_key: integer (nullable = true)\n",
      " |-- product_id: double (nullable = true)\n",
      " |-- product_number: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- color: string (nullable = true)\n",
      "\n",
      "Sample rows:\n",
      "+-----------+----------+--------------+-----------------------+-----+\n",
      "|product_key|product_id|product_number|name                   |color|\n",
      "+-----------+----------+--------------+-----------------------+-----+\n",
      "|1          |NULL      |708           |Sport-100 Helmet, Black|Black|\n",
      "|2          |NULL      |709           |Mountain Bike Socks, M |White|\n",
      "|3          |NULL      |710           |Mountain Bike Socks, L |White|\n",
      "|4          |NULL      |711           |Sport-100 Helmet, Blue |Blue |\n",
      "|5          |NULL      |712           |AWC Logo Cap           |Multi|\n",
      "+-----------+----------+--------------+-----------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=== Table: dim_shipping_region ===\n",
      "Schema:\n",
      "root\n",
      " |-- shippingregion_key: integer (nullable = true)\n",
      " |-- shippingregion_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "Sample rows:\n",
      "+------------------+-----------------+-------------+-------+\n",
      "|shippingregion_key|shippingregion_id|name         |country|\n",
      "+------------------+-----------------+-------------+-------+\n",
      "|1                 |2                |West Coast   |USA    |\n",
      "|2                 |3                |Midwest      |USA    |\n",
      "|3                 |4                |South        |USA    |\n",
      "|4                 |5                |International|Global |\n",
      "+------------------+-----------------+-------------+-------+\n",
      "\n",
      "\n",
      "=== Table: dim_date ===\n",
      "Schema:\n",
      "root\n",
      " |-- date_key_surrogate: integer (nullable = true)\n",
      " |-- date_key: integer (nullable = true)\n",
      " |-- full_date: date (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: long (nullable = true)\n",
      "\n",
      "Sample rows:\n",
      "+------------------+--------+----------+---+-----+----+-------+\n",
      "|date_key_surrogate|date_key|full_date |day|month|year|quarter|\n",
      "+------------------+--------+----------+---+-----+----+-------+\n",
      "|1                 |20010701|2001-07-01|1  |1    |2001|1      |\n",
      "|2                 |20010702|2001-07-02|2  |0    |2001|0      |\n",
      "|3                 |20010703|2001-07-03|3  |0    |2001|0      |\n",
      "|4                 |20010704|2001-07-04|4  |0    |2001|0      |\n",
      "|5                 |20010705|2001-07-05|5  |0    |2001|0      |\n",
      "+------------------+--------+----------+---+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=== Table: dim_culture ===\n",
      "Schema:\n",
      "root\n",
      " |-- CurrencyCode: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- ModifiedDate: timestamp (nullable = true)\n",
      "\n",
      "Sample rows:\n",
      "+------------+-----------------------------+-------------------+\n",
      "|CurrencyCode|Name                         |ModifiedDate       |\n",
      "+------------+-----------------------------+-------------------+\n",
      "|AED         |Emirati Dirham               |1998-06-01 00:00:00|\n",
      "|AFA         |Afghani                      |1998-06-01 00:00:00|\n",
      "|ALL         |Lek                          |1998-06-01 00:00:00|\n",
      "|AMD         |Armenian Dram                |1998-06-01 00:00:00|\n",
      "|ANG         |Netherlands Antillian Guilder|1998-06-01 00:00:00|\n",
      "+------------+-----------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "=== Table: dim_currency ===\n",
      "Schema:\n",
      "root\n",
      " |-- CurrencyCode: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- ModifiedDate: timestamp (nullable = true)\n",
      "\n",
      "Sample rows:\n",
      "+------------+-----------------------------+-------------------+\n",
      "|CurrencyCode|Name                         |ModifiedDate       |\n",
      "+------------+-----------------------------+-------------------+\n",
      "|AED         |Emirati Dirham               |1998-06-01 00:00:00|\n",
      "|AFA         |Afghani                      |1998-06-01 00:00:00|\n",
      "|ALL         |Lek                          |1998-06-01 00:00:00|\n",
      "|AMD         |Armenian Dram                |1998-06-01 00:00:00|\n",
      "|ANG         |Netherlands Antillian Guilder|1998-06-01 00:00:00|\n",
      "+------------+-----------------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dim_tables = [\n",
    "    \"dim_customer\",\n",
    "    \"dim_product\",\n",
    "    \"dim_shipping_region\",\n",
    "    \"dim_date\", \n",
    "    \"dim_culture\", \n",
    "    \"dim_currency\", \n",
    "    \n",
    "]\n",
    "\n",
    "dest_database = \"estella_dw\"\n",
    "\n",
    "for table in dim_tables:\n",
    "    print(f\"\\n=== Table: {table} ===\")\n",
    "    df = spark.table(f\"{dest_database}.{table}\")\n",
    "    print(\"Schema:\")\n",
    "    df.printSchema()\n",
    "    print(\"Sample rows:\")\n",
    "    df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d475d6-af22-44d8-a0b3-bde20491a85c",
   "metadata": {},
   "source": [
    "#### 6.3. Create the Silver Layer: Integrate \"Cold-path\" Data & Make Transformations\n",
    "##### 6.3.1. Prepare Role-Playing Dimension Primary and Business Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a2fb4a1-660c-4979-8acd-087b658e0d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension tables prepared with primary and business keys.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_dim_product = df_dim_product.withColumnRenamed(\"id\", \"product_key\")\n",
    "df_dim_product = df_dim_product.withColumnRenamed(\"product_number\", \"product_business_key\")\n",
    "\n",
    "df_dim_customer = df_dim_customer.withColumnRenamed(\"id\", \"customer_key\")\n",
    "df_dim_customer = df_dim_customer.withColumnRenamed(\"customer_id\", \"customer_id_orig\")\n",
    "\n",
    "df_dim_date = df_dim_date.withColumnRenamed(\"id\", \"date_key\")\n",
    "df_dim_date = df_dim_date.withColumnRenamed(\"full_date\", \"date_full_date\")\n",
    "\n",
    "df_dim_shipping_region = df_dim_shipping_region.withColumnRenamed(\"id\", \"shippingregion_key\")\n",
    "\n",
    "print(\"Dimension tables prepared with primary and business keys.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7769a299-2802-43e9-b6a0-7351d98bb04d",
   "metadata": {},
   "source": [
    "##### 6.3.2. Define Silver Query to Join Streaming with Batch Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b301397-bec4-456d-bac7-5bb1093dbe50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is streaming? True\n"
     ]
    }
   ],
   "source": [
    "fact_sales_output_bronze = r\"C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\streaming\\fact_sales\\Bronze\"\n",
    "\n",
    "\n",
    "df_bronze_stream = spark.readStream.format(\"parquet\").load(fact_sales_output_bronze)\n",
    "\n",
    "\n",
    "df_silver_stream = (\n",
    "    df_bronze_stream\n",
    "    .join(\n",
    "        df_dim_product,\n",
    "        df_bronze_stream.product_id.cast(\"integer\") == df_dim_product.product_business_key,\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_dim_date,\n",
    "        df_bronze_stream.date_key == df_dim_date.date_key,\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_dim_customer,\n",
    "        df_bronze_stream.customer_id == df_dim_customer.customer_id_orig,\n",
    "        \"left\"\n",
    "    )\n",
    "    .join(\n",
    "        df_dim_shipping_region,\n",
    "        df_bronze_stream.region_id == df_dim_shipping_region.shippingregion_key,\n",
    "        \"left\"\n",
    "    )\n",
    "    .select(\n",
    "        df_bronze_stream.order_id,\n",
    "        df_bronze_stream.order_line_id,\n",
    "        df_bronze_stream.customer_id,\n",
    "        df_bronze_stream.product_id.alias(\"bronze_product_id\"),\n",
    "        df_dim_product.product_key,\n",
    "        df_dim_product.color.alias(\"product_category\"),\n",
    "        df_dim_product.name.alias(\"product_name\"),\n",
    "        df_bronze_stream.quantity,\n",
    "        df_bronze_stream.unit_price,\n",
    "        df_bronze_stream.extended_amount,\n",
    "        df_bronze_stream.date_key.alias(\"order_date_key\"),\n",
    "        df_dim_date.date_full_date.alias(\"full_date\"),\n",
    "        df_dim_date.month,\n",
    "        df_dim_date.year,\n",
    "        df_dim_date.quarter,\n",
    "        df_bronze_stream.region_id,\n",
    "        df_dim_shipping_region.name.alias(\"region_name\"),\n",
    "        df_dim_customer.full_name.alias(\"customer_name\"),\n",
    "        df_bronze_stream.receipt_time,\n",
    "        df_bronze_stream.source_file\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Is streaming? {df_silver_stream.isStreaming}\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2143c4-f641-4070-a5b8-fd78dcc0b0c7",
   "metadata": {},
   "source": [
    "#### 6.3.3. Write the Transformed Streaming data to the Data Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0bac853-37c9-4a4a-a461-09da80aba400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Silver query started. Use df_silver_stream_query.awaitTermination() to keep it running.\n"
     ]
    }
   ],
   "source": [
    "fact_sales_output_silver = r\"C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\streaming\\fact_sales\\Silver\"\n",
    "checkpoint_location = r\"C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\streaming\\fact_sales\\Silver_checkpoint\"\n",
    "\n",
    "\n",
    "df_silver_stream_query = (\n",
    "    df_silver_stream.writeStream\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", fact_sales_output_silver)\n",
    "    .option(\"checkpointLocation\", checkpoint_location)\n",
    "    .outputMode(\"append\")  \n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"Streaming Silver query started. Use df_silver_stream_query.awaitTermination() to keep it running.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8fc0f3cc-cbc9-454b-b093-f74ff4694a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_silver_stream_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3aeac2c-d998-4def-b613-2fc2d1d52041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verified Silver schema:\n",
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_line_id: long (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- bronze_product_id: long (nullable = true)\n",
      " |-- product_key: integer (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- quantity: long (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- extended_amount: double (nullable = true)\n",
      " |-- order_date_key: long (nullable = true)\n",
      " |-- full_date: date (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: long (nullable = true)\n",
      " |-- region_id: long (nullable = true)\n",
      " |-- region_name: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- receipt_time: timestamp (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      "\n",
      "\n",
      "Sample rows from Silver:\n",
      "+--------+-------------+-----------+-----------------+-----------+----------------+------------------------------+--------+----------+------------------+--------------+----------+-----+----+-------+---------+-----------+-------------+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|order_id|order_line_id|customer_id|bronze_product_id|product_key|product_category|product_name                  |quantity|unit_price|extended_amount   |order_date_key|full_date |month|year|quarter|region_id|region_name|customer_name|receipt_time           |source_file                                                                                                                                   |\n",
      "+--------+-------------+-----------+-----------------+-----------+----------------+------------------------------+--------+----------+------------------+--------------+----------+-----+----+-------+---------+-----------+-------------+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|43666   |73           |511        |732              |20         |Red             |ML Road Frame - Red, 48       |1       |356.898   |356.898           |20010701      |2001-07-01|1    |2001|1      |2        |West Coast |Unknown      |2025-12-17 17:00:46.509|file:///C:/Users/stell/OneDrive/Desktop/Documents/DS-2002-main/04-PySpark/lab_data/estella_dw/streaming/fact_sales/fact_sales_43659_54147.json|\n",
      "|43855   |583          |102        |732              |20         |Red             |ML Road Frame - Red, 48       |2       |356.898   |713.796           |20010801      |2001-08-01|0    |2001|0      |2        |West Coast |Unknown      |2025-12-17 17:00:46.509|file:///C:/Users/stell/OneDrive/Desktop/Documents/DS-2002-main/04-PySpark/lab_data/estella_dw/streaming/fact_sales/fact_sales_43659_54147.json|\n",
      "|43695   |345          |27         |748              |31         |Silver          |HL Mountain Frame - Silver, 38|2       |722.5949  |1445.1898         |20010701      |2001-07-01|1    |2001|1      |2        |West Coast |Unknown      |2025-12-17 17:00:46.509|file:///C:/Users/stell/OneDrive/Desktop/Documents/DS-2002-main/04-PySpark/lab_data/estella_dw/streaming/fact_sales/fact_sales_43659_54147.json|\n",
      "|43867   |714          |145        |748              |31         |Silver          |HL Mountain Frame - Silver, 38|1       |722.5949  |722.5949          |20010801      |2001-08-01|0    |2001|0      |2        |West Coast |Unknown      |2025-12-17 17:00:46.509|file:///C:/Users/stell/OneDrive/Desktop/Documents/DS-2002-main/04-PySpark/lab_data/estella_dw/streaming/fact_sales/fact_sales_43659_54147.json|\n",
      "|43875   |802          |278        |748              |31         |Silver          |HL Mountain Frame - Silver, 38|5       |722.5949  |3612.9745000000003|20010801      |2001-08-01|0    |2001|0      |2        |West Coast |Unknown      |2025-12-17 17:00:46.509|file:///C:/Users/stell/OneDrive/Desktop/Documents/DS-2002-main/04-PySpark/lab_data/estella_dw/streaming/fact_sales/fact_sales_43659_54147.json|\n",
      "|43873   |772          |78         |766              |49         |Black           |Road-650 Black, 60            |3       |419.4589  |1258.3767         |20010801      |2001-08-01|0    |2001|0      |2        |West Coast |Unknown      |2025-12-17 17:00:46.509|file:///C:/Users/stell/OneDrive/Desktop/Documents/DS-2002-main/04-PySpark/lab_data/estella_dw/streaming/fact_sales/fact_sales_43659_54147.json|\n",
      "|43880   |865          |336        |766              |49         |Black           |Road-650 Black, 60            |1       |419.4589  |419.4589          |20010801      |2001-08-01|0    |2001|0      |2        |West Coast |Unknown      |2025-12-17 17:00:46.509|file:///C:/Users/stell/OneDrive/Desktop/Documents/DS-2002-main/04-PySpark/lab_data/estella_dw/streaming/fact_sales/fact_sales_43659_54147.json|\n",
      "|43668   |94           |514        |770              |53         |Black           |Road-650 Black, 52            |2       |419.4589  |838.9178          |20010701      |2001-07-01|1    |2001|1      |2        |West Coast |Unknown      |2025-12-17 17:00:46.509|file:///C:/Users/stell/OneDrive/Desktop/Documents/DS-2002-main/04-PySpark/lab_data/estella_dw/streaming/fact_sales/fact_sales_43659_54147.json|\n",
      "|43681   |210          |423        |770              |53         |Black           |Road-650 Black, 52            |3       |419.4589  |1258.3767         |20010701      |2001-07-01|1    |2001|1      |2        |West Coast |Unknown      |2025-12-17 17:00:46.509|file:///C:/Users/stell/OneDrive/Desktop/Documents/DS-2002-main/04-PySpark/lab_data/estella_dw/streaming/fact_sales/fact_sales_43659_54147.json|\n",
      "|43851   |560          |558        |770              |53         |Black           |Road-650 Black, 52            |3       |419.4589  |1258.3767         |20010801      |2001-08-01|0    |2001|0      |2        |West Coast |Unknown      |2025-12-17 17:00:46.509|file:///C:/Users/stell/OneDrive/Desktop/Documents/DS-2002-main/04-PySpark/lab_data/estella_dw/streaming/fact_sales/fact_sales_43659_54147.json|\n",
      "+--------+-------------+-----------+-----------------+-----------+----------------+------------------------------+--------+----------+------------------+--------------+----------+-----+----+-------+---------+-----------+-------------+-----------------------+----------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+----------------+-----+----+\n",
      "|product_key|product_category|month|year|\n",
      "+-----------+----------------+-----+----+\n",
      "|        118|             118|    0|   0|\n",
      "+-----------+----------------+-----+----+\n",
      "\n",
      "+----------------+-----+\n",
      "|product_category|count|\n",
      "+----------------+-----+\n",
      "|\\N              |1076 |\n",
      "|Black           |555  |\n",
      "|Red             |445  |\n",
      "|Multi           |251  |\n",
      "|Silver          |207  |\n",
      "|Blue            |159  |\n",
      "|Yellow          |147  |\n",
      "|NULL            |118  |\n",
      "|White           |42   |\n",
      "+----------------+-----+\n",
      "\n",
      "\n",
      "Silver layer monitoring complete.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum, when, isnull\n",
    "\n",
    "df_silver_stream_query.awaitTermination()\n",
    "\n",
    "df_verify = spark.read.parquet(fact_sales_output_silver)\n",
    "\n",
    "print(\"\\nVerified Silver schema:\")\n",
    "df_verify.printSchema()\n",
    "\n",
    "print(\"\\nSample rows from Silver:\")\n",
    "df_verify.show(10, truncate=False)\n",
    "\n",
    "df_verify.select([\n",
    "    _sum(when(isnull(c), 1).otherwise(0)).alias(c)\n",
    "    for c in [\"product_key\", \"product_category\", \"month\", \"year\"]\n",
    "]).show()\n",
    "\n",
    "df_verify.groupBy(\"product_category\").count().orderBy(col(\"count\").desc()).show(20, truncate=False)\n",
    "\n",
    "print(\"\\nSilver layer monitoring complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f73965-bb8b-498a-8498-786c5cf5352a",
   "metadata": {},
   "source": [
    "#### 6.4. Create Gold Layer: Perform Aggregations\n",
    "##### 6.4.1. Define a Query to Create a Business Report\n",
    "Create a new Gold table using the PySpark API. The table should include the number of Products sold per Category each Month. The results should include The Month, Product Category and Number of Products sold, sorted by the month number when the orders were placed: e.g., January, February, March."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5100f935-5f16-4854-91d5-1fe3e1204484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum, when\n",
    "import os\n",
    "\n",
    "fact_sales_output_silver = r\"C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\streaming\\fact_sales\\Silver\"\n",
    "fact_sales_output_gold = r\"C:\\Users\\stell\\OneDrive\\Desktop\\Documents\\DS-2002-main\\04-PySpark\\lab_data\\estella_dw\\streaming\\fact_sales\\Gold\"\n",
    "\n",
    "for stream in spark.streams.active:\n",
    "    if stream.name in [\"fact_sales_gold\", \"fact_sales_gold_memory\"]:\n",
    "        stream.stop()\n",
    "\n",
    "df_silver_stream = spark.readStream.option(\"maxFilesPerTrigger\", 1).parquet(fact_sales_output_silver)\n",
    "\n",
    "df_gold_stream = (\n",
    "    df_silver_stream\n",
    "    .filter(col(\"product_category\").isNotNull() & col(\"month\").isNotNull() & col(\"year\").isNotNull() & col(\"quantity\").isNotNull())\n",
    "    .withColumn(\"category_clean\", when(col(\"product_category\").isNull(), \"Unknown\").otherwise(col(\"product_category\")))\n",
    "    .groupBy(\"month\", \"year\", \"category_clean\")\n",
    "    .agg(_sum(\"quantity\").alias(\"product_count\"))\n",
    "    .select(col(\"month\").alias(\"Month\"), col(\"year\").alias(\"Year\"), col(\"category_clean\").alias(\"Product_Category\"), col(\"product_count\").alias(\"Product_Count\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62735a13-01f5-47c7-a7c8-5b71e682c776",
   "metadata": {},
   "source": [
    "#### 6.4.2: Write Streaming data to MEMORY (Complete mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5bb1a987-8639-47d1-9412-4a44e4eec8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fact_sales_gold_memory_query = (\n",
    "    df_gold_stream.writeStream\n",
    "    .format(\"memory\")\n",
    "    .outputMode(\"complete\")\n",
    "    .queryName(\"fact_sales_gold_memory\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "fact_sales_gold_memory_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df29b58-4709-4147-bc1d-535672323bfa",
   "metadata": {},
   "source": [
    "#### 6.4.3: Query the Gold Data from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4719145b-e5a9-4451-bfa1-b4254914b7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------------+-------------+\n",
      "|Month|Year|Product_Category|Product_Count|\n",
      "+-----+----+----------------+-------------+\n",
      "|0    |2001|Red             |463          |\n",
      "|0    |2001|Black           |370          |\n",
      "|0    |2001|Silver          |209          |\n",
      "|0    |2001|Multi           |176          |\n",
      "|0    |2001|White           |69           |\n",
      "|0    |2001|Blue            |34           |\n",
      "|1    |2001|Red             |360          |\n",
      "|1    |2001|Black           |236          |\n",
      "|1    |2001|Multi           |124          |\n",
      "|1    |2001|Silver          |50           |\n",
      "|1    |2001|White           |43           |\n",
      "|1    |2001|Blue            |33           |\n",
      "|0    |2003|\\N              |398          |\n",
      "|0    |2003|Black           |91           |\n",
      "|0    |2003|Yellow          |53           |\n",
      "|0    |2003|Multi           |47           |\n",
      "|0    |2003|Blue            |38           |\n",
      "|0    |2003|Silver          |28           |\n",
      "|0    |2003|White           |8            |\n",
      "|0    |2003|Red             |1            |\n",
      "|1    |2003|\\N              |161          |\n",
      "|1    |2003|Black           |40           |\n",
      "|1    |2003|Yellow          |23           |\n",
      "|1    |2003|Multi           |22           |\n",
      "|1    |2003|Blue            |21           |\n",
      "|1    |2003|Silver          |16           |\n",
      "|1    |2003|White           |2            |\n",
      "|0    |2004|\\N              |372          |\n",
      "|0    |2004|Black           |106          |\n",
      "|0    |2004|Multi           |58           |\n",
      "|0    |2004|Yellow          |54           |\n",
      "|0    |2004|Blue            |44           |\n",
      "|0    |2004|Silver          |32           |\n",
      "|0    |2004|White           |5            |\n",
      "|0    |2004|Red             |2            |\n",
      "|1    |2004|\\N              |145          |\n",
      "|1    |2004|Black           |43           |\n",
      "|1    |2004|Blue            |30           |\n",
      "|1    |2004|Multi           |25           |\n",
      "|1    |2004|Silver          |21           |\n",
      "|1    |2004|Yellow          |17           |\n",
      "|1    |2004|Red             |1            |\n",
      "|1    |2004|White           |1            |\n",
      "+-----+----+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_gold_memory = spark.sql(\"SELECT * FROM fact_sales_gold_memory ORDER BY Year, Month, Product_Count DESC\")\n",
    "df_gold_memory.show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8282e3-26b7-43fd-b33f-403e2edc8a64",
   "metadata": {},
   "source": [
    "#### 6.4.4: Create the Final Selection / Write to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea5c196a-a490-43e2-8ce9-7c569488926f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------------+-------------+\n",
      "|Month|Year|Product_Category|Product_Count|\n",
      "+-----+----+----------------+-------------+\n",
      "|0    |2001|Red             |463          |\n",
      "|0    |2001|Black           |370          |\n",
      "|0    |2001|Silver          |209          |\n",
      "|0    |2001|Multi           |176          |\n",
      "|0    |2001|White           |69           |\n",
      "|0    |2001|Blue            |34           |\n",
      "|1    |2001|Red             |360          |\n",
      "|1    |2001|Black           |236          |\n",
      "|1    |2001|Multi           |124          |\n",
      "|1    |2001|Silver          |50           |\n",
      "|1    |2001|White           |43           |\n",
      "|1    |2001|Blue            |33           |\n",
      "|0    |2003|\\N              |398          |\n",
      "|0    |2003|Black           |91           |\n",
      "|0    |2003|Yellow          |53           |\n",
      "|0    |2003|Multi           |47           |\n",
      "|0    |2003|Blue            |38           |\n",
      "|0    |2003|Silver          |28           |\n",
      "|0    |2003|White           |8            |\n",
      "|0    |2003|Red             |1            |\n",
      "|1    |2003|\\N              |161          |\n",
      "|1    |2003|Black           |40           |\n",
      "|1    |2003|Yellow          |23           |\n",
      "|1    |2003|Multi           |22           |\n",
      "|1    |2003|Blue            |21           |\n",
      "|1    |2003|Silver          |16           |\n",
      "|1    |2003|White           |2            |\n",
      "|0    |2004|\\N              |372          |\n",
      "|0    |2004|Black           |106          |\n",
      "|0    |2004|Multi           |58           |\n",
      "|0    |2004|Yellow          |54           |\n",
      "|0    |2004|Blue            |44           |\n",
      "|0    |2004|Silver          |32           |\n",
      "|0    |2004|White           |5            |\n",
      "|0    |2004|Red             |2            |\n",
      "|1    |2004|\\N              |145          |\n",
      "|1    |2004|Black           |43           |\n",
      "|1    |2004|Blue            |30           |\n",
      "|1    |2004|Multi           |25           |\n",
      "|1    |2004|Silver          |21           |\n",
      "|1    |2004|Yellow          |17           |\n",
      "|1    |2004|Red             |1            |\n",
      "|1    |2004|White           |1            |\n",
      "+-----+----+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_gold_memory.write.mode(\"overwrite\").parquet(fact_sales_output_gold)\n",
    "\n",
    "df_gold_verify = spark.read.parquet(fact_sales_output_gold)\n",
    "df_gold_verify.orderBy(\"Year\", \"Month\", col(\"Product_Count\").desc()).show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc359de-da5a-43f4-88e6-092ae51158cb",
   "metadata": {},
   "source": [
    "#### 6.4.5: Load into Table and Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "46b0ba0e-0b6b-4659-9277-decbdbc8666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----------------+-------------+\n",
      "|Month|Year|Product_Category|Product_Count|\n",
      "+-----+----+----------------+-------------+\n",
      "|0    |2001|Red             |463          |\n",
      "|0    |2001|Black           |370          |\n",
      "|0    |2001|Silver          |209          |\n",
      "|0    |2001|Multi           |176          |\n",
      "|0    |2001|White           |69           |\n",
      "|0    |2001|Blue            |34           |\n",
      "|1    |2001|Red             |360          |\n",
      "|1    |2001|Black           |236          |\n",
      "|1    |2001|Multi           |124          |\n",
      "|1    |2001|Silver          |50           |\n",
      "|1    |2001|White           |43           |\n",
      "|1    |2001|Blue            |33           |\n",
      "|0    |2003|\\N              |398          |\n",
      "|0    |2003|Black           |91           |\n",
      "|0    |2003|Yellow          |53           |\n",
      "|0    |2003|Multi           |47           |\n",
      "|0    |2003|Blue            |38           |\n",
      "|0    |2003|Silver          |28           |\n",
      "|0    |2003|White           |8            |\n",
      "|0    |2003|Red             |1            |\n",
      "|1    |2003|\\N              |161          |\n",
      "|1    |2003|Black           |40           |\n",
      "|1    |2003|Yellow          |23           |\n",
      "|1    |2003|Multi           |22           |\n",
      "|1    |2003|Blue            |21           |\n",
      "|1    |2003|Silver          |16           |\n",
      "|1    |2003|White           |2            |\n",
      "|0    |2004|\\N              |372          |\n",
      "|0    |2004|Black           |106          |\n",
      "|0    |2004|Multi           |58           |\n",
      "|0    |2004|Yellow          |54           |\n",
      "|0    |2004|Blue            |44           |\n",
      "|0    |2004|Silver          |32           |\n",
      "|0    |2004|White           |5            |\n",
      "|0    |2004|Red             |2            |\n",
      "|1    |2004|\\N              |145          |\n",
      "|1    |2004|Black           |43           |\n",
      "|1    |2004|Blue            |30           |\n",
      "|1    |2004|Multi           |25           |\n",
      "|1    |2004|Silver          |21           |\n",
      "|1    |2004|Yellow          |17           |\n",
      "|1    |2004|Red             |1            |\n",
      "|1    |2004|White           |1            |\n",
      "+-----+----+----------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dest_database = \"estella_dw\"\n",
    "\n",
    "try:\n",
    "    df_gold_verify.write.mode(\"overwrite\").saveAsTable(f\"{dest_database}.fact_products_sold_by_month\")\n",
    "    result = spark.sql(f\"SELECT * FROM {dest_database}.fact_products_sold_by_month ORDER BY Year, Month, Product_Count DESC\")\n",
    "    result.show(50, truncate=False)\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d65c9-d839-4e48-b3b6-120c18091506",
   "metadata": {},
   "source": [
    "### 7 Stop the Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bdff8b-6e80-4c1b-bb51-5f7536f7ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (pysparkenv)",
   "language": "python",
   "name": "pysparkenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
